{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORKrfqz6gfVoXG6iUQhNQH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PravalikaMuthoju/INTERNSHIP-PROJECTS/blob/main/JSSP_Via_DRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN-DgCMia7Y8",
        "outputId": "e2212ee6-7a4d-44da-9214-61cac8733e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Makespan: 45, Epsilon: 0.995\n",
            "Episode 1000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 2000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 3000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 4000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 5000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 6000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 7000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 8000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 9000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 10000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 11000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 12000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 13000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 14000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 15000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 16000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 17000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 18000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 19000, Makespan: 45, Epsilon: 0.050\n",
            "\n",
            "ðŸ”§ Job-Machine-Time Assignments:\n",
            " Job ID  Operation  Machine  Start  End  Current Makespan  Contributes to Makespan\n",
            "      0          0        1      0    1                 1                     True\n",
            "      5          0        2      0    3                 3                     True\n",
            "      1          0        3      0    8                 8                     True\n",
            "      5          1        1      1    4                 8                    False\n",
            "      5          2        0      0    9                 9                     True\n",
            "      1          1        2      3    8                 9                    False\n",
            "      3          0        1      4    9                 9                     True\n",
            "      2          0        2      8   13                13                     True\n",
            "      3          1        3      8   13                13                     True\n",
            "      3          2        4      0    5                13                    False\n",
            "      1          2        1      9   19                19                     True\n",
            "      1          3        0      9   19                19                     True\n",
            "      1          4        5      0   10                19                    False\n",
            "      1          5        4      5    9                19                    False\n",
            "      5          3        3     13   23                23                     True\n",
            "      5          4        4      9   13                23                    False\n",
            "      5          5        5     10   11                23                    False\n",
            "      3          3        2     13   16                23                    False\n",
            "      0          1        2     16   19                23                    False\n",
            "      3          4        0     19   27                27                     True\n",
            "      3          5        5     11   20                27                    False\n",
            "      2          1        1     19   23                27                    False\n",
            "      4          0        3     23   32                32                     True\n",
            "      4          1        2     19   22                32                    False\n",
            "      4          2        5     20   25                32                    False\n",
            "      2          2        0     27   35                35                     True\n",
            "      2          3        5     25   34                35                    False\n",
            "      0          2        3     32   38                38                     True\n",
            "      0          3        4     13   20                38                    False\n",
            "      0          4        5     34   37                38                    False\n",
            "      4          3        0     35   39                39                     True\n",
            "      4          4        1     23   26                39                    False\n",
            "      4          5        4     20   21                39                    False\n",
            "      2          4        3     38   39                39                     True\n",
            "      2          5        4     21   28                39                    False\n",
            "      0          5        0     39   45                45                     True\n",
            "\n",
            "ðŸš€ Initial Random Makespan: 45\n",
            "ðŸ“Š Total Optimized Makespan: 45\n",
            "ðŸ“‰ Makespan Reduced By: 0 time units\n",
            "ðŸ›  Idle Machine Time Units: 73\n",
            "ðŸ“ˆ Machine Utilization: 72.96%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "# Load FT10 Dataset\n",
        "df = pd.read_csv(\"/content/FT06_Dataset.csv\")\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "# Convert machine names like 'M1' to integers\n",
        "df[\"Machine\"] = df[\"Machine\"].apply(lambda x: int(x[1:]) if isinstance(x, str) and x.startswith(\"M\") else int(x))\n",
        "df.rename(columns={\"Job ID\": \"Job\", \"Operation\": \"Op\", \"Machine\": \"Machine\", \"Processing Time\": \"Time\"}, inplace=True)\n",
        "\n",
        "num_jobs = df[\"Job\"].nunique()\n",
        "num_machines = df[\"Machine\"].nunique()\n",
        "job_ops = df.groupby(\"Job\")[\"Op\"].max().to_dict()\n",
        "job_ops = {j: v + 1 for j, v in job_ops.items()}\n",
        "job_data = {(int(r[\"Job\"]), int(r[\"Op\"])): (int(r[\"Machine\"]), int(r[\"Time\"])) for _, r in df.iterrows()}\n",
        "\n",
        "# Deep Q-Network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Settings\n",
        "gamma = 0.95\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 0.995\n",
        "alpha = 0.001\n",
        "episodes = 20000 # Increased episodes\n",
        "batch_size = 128\n",
        "memory = deque(maxlen=50000)\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size = num_jobs + num_machines\n",
        "output_size = num_jobs\n",
        "\n",
        "q_net = QNetwork(input_size, output_size).to(device)\n",
        "optimizer = optim.Adam(q_net.parameters(), lr=alpha)\n",
        "\n",
        "# Helper functions\n",
        "def get_state_vector(job_progress, machine_status):\n",
        "    return np.array(job_progress + machine_status, dtype=np.float32)\n",
        "\n",
        "def get_feasible_actions(job_progress, machine_status, time):\n",
        "    actions = []\n",
        "    for j in range(num_jobs):\n",
        "        op = job_progress[j]\n",
        "        if op < job_ops[j]:\n",
        "            m, pt = job_data[(j, op)]\n",
        "            if machine_status[m] <= time:\n",
        "                actions.append(j)\n",
        "    return actions\n",
        "\n",
        "def simulate_episode(train=True):\n",
        "    job_progress = [0] * num_jobs\n",
        "    machine_status = [0] * num_machines\n",
        "    schedule = []\n",
        "    time = 0\n",
        "    total_reward = 0\n",
        "\n",
        "    while any(job_progress[j] < job_ops[j] for j in range(num_jobs)):\n",
        "        state = get_state_vector(job_progress, machine_status)\n",
        "        feasible = get_feasible_actions(job_progress, machine_status, time)\n",
        "\n",
        "        if not feasible:\n",
        "            next_event_time = float('inf')\n",
        "            for ms in machine_status:\n",
        "                if ms > time:\n",
        "                    next_event_time = min(next_event_time, ms)\n",
        "            if next_event_time == float('inf'):\n",
        "                if all(job_progress[j] >= job_ops[j] for j in range(num_jobs)):\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"Error: No feasible actions but jobs are not completed.\")\n",
        "                    break\n",
        "            time = next_event_time\n",
        "            continue\n",
        "\n",
        "        state_tensor = torch.tensor(state, device=device)\n",
        "\n",
        "        if train and random.random() < epsilon:\n",
        "            action_job = random.choice(feasible)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_vals = q_net(state_tensor).cpu().numpy()\n",
        "            masked_q = np.full_like(q_vals, -np.inf)\n",
        "            for j in feasible:\n",
        "                masked_q[j] = q_vals[j]\n",
        "            action_job = int(np.argmax(masked_q))\n",
        "\n",
        "        op = job_progress[action_job]\n",
        "        m, pt = job_data[(action_job, op)]\n",
        "        start = max(machine_status[m], time)\n",
        "        end = start + pt\n",
        "\n",
        "        # Modified Reward Function: Penalize makespan and idle time\n",
        "        current_makespan = max(end, max(machine_status)) # Consider potential makespan increase\n",
        "        reward = -pt - (current_makespan - time) * 0.1 # Penalize processing time and time until next job can start\n",
        "\n",
        "\n",
        "        next_job_progress = job_progress.copy()\n",
        "        next_machine_status = machine_status.copy()\n",
        "        next_job_progress[action_job] += 1\n",
        "        next_machine_status[m] = end\n",
        "        next_state = get_state_vector(next_job_progress, next_machine_status)\n",
        "\n",
        "        if train:\n",
        "            memory.append(Transition(state, action_job, reward, next_state))\n",
        "\n",
        "        job_progress = next_job_progress\n",
        "        machine_status = next_machine_status\n",
        "        time = min(machine_status)\n",
        "        schedule.append((action_job, op, m, start, end))\n",
        "        total_reward += reward\n",
        "\n",
        "    makespan = max(e for *_, e in schedule) if schedule else 0\n",
        "    return total_reward, makespan, schedule\n",
        "\n",
        "# Training loop\n",
        "makespans = []\n",
        "schedules = []\n",
        "# Run one episode with pure random actions to get a baseline initial makespan\n",
        "_, initial_makespan, _ = simulate_episode(train=False)\n",
        "makespans.append(initial_makespan)\n",
        "\n",
        "for ep in range(episodes):\n",
        "    _, mkspan, sched = simulate_episode(train=True)\n",
        "    makespans.append(mkspan)\n",
        "    schedules.append(sched)\n",
        "\n",
        "    if len(memory) >= batch_size:\n",
        "        batch = random.sample(memory, batch_size)\n",
        "        batch = Transition(*zip(*batch))\n",
        "\n",
        "        state_batch = torch.tensor(batch.state, device=device)\n",
        "        action_batch = torch.tensor(batch.action, device=device).unsqueeze(1)\n",
        "        reward_batch = torch.tensor(batch.reward, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "        next_state_batch = torch.tensor(batch.next_state, device=device)\n",
        "\n",
        "        q_values = q_net(state_batch).gather(1, action_batch)\n",
        "        with torch.no_grad():\n",
        "            q_next = q_net(next_state_batch).max(1)[0].unsqueeze(1)\n",
        "        target = reward_batch + gamma * q_next\n",
        "\n",
        "        loss = nn.MSELoss()(q_values, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "    if ep % 1000 == 0: # Print less often for more episodes\n",
        "        print(f\"Episode {ep}, Makespan: {mkspan}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "# Best result\n",
        "best_idx = np.argmin(makespans)\n",
        "best_makespan = makespans[best_idx]\n",
        "best_schedule = schedules[best_idx]\n",
        "\n",
        "print(\"\\nðŸ”§ Job-Machine-Time Assignments:\")\n",
        "schedule_df = pd.DataFrame(best_schedule, columns=[\"Job ID\", \"Operation\", \"Machine\", \"Start\", \"End\"])\n",
        "schedule_df[\"Current Makespan\"] = schedule_df[\"End\"].cummax()\n",
        "schedule_df[\"Contributes to Makespan\"] = schedule_df[\"End\"] == schedule_df[\"Current Makespan\"]\n",
        "print(schedule_df.to_string(index=False))\n",
        "\n",
        "initial_makespan = makespans[0]\n",
        "makespan_reduced_by = initial_makespan - best_makespan\n",
        "total_processing_time = sum(df[\"Time\"])\n",
        "total_machine_time = best_makespan * num_machines\n",
        "idle_time = total_machine_time - total_processing_time\n",
        "machine_utilization = (total_processing_time / total_machine_time) * 100 if total_machine_time > 0 else 0\n",
        "\n",
        "print(f\"\\nðŸš€ Initial Random Makespan: {initial_makespan}\")\n",
        "print(f\"ðŸ“Š Total Optimized Makespan: {best_makespan}\")\n",
        "print(f\"ðŸ“‰ Makespan Reduced By: {makespan_reduced_by} time units\")\n",
        "print(f\"ðŸ›  Idle Machine Time Units: {idle_time}\")\n",
        "print(f\"ðŸ“ˆ Machine Utilization: {machine_utilization:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Improvements based on your request:\n",
        "# - Accurately compare initial vs optimized makespan\n",
        "# - Compute real machine usage time (not just sum of all processing times)\n",
        "# - Show only final makespan-contributing operations\n",
        "# - Make output easier to analyze\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/FT06_Dataset.csv\")\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "df[\"Machine\"] = df[\"Machine\"].apply(lambda x: int(x[1:]) if isinstance(x, str) and x.startswith(\"M\") else int(x))\n",
        "df.rename(columns={\"Job ID\": \"Job\", \"Operation\": \"Op\", \"Machine\": \"Machine\", \"Processing Time\": \"Time\"}, inplace=True)\n",
        "\n",
        "num_jobs = df[\"Job\"].nunique()\n",
        "num_machines = df[\"Machine\"].nunique()\n",
        "job_ops = df.groupby(\"Job\")[\"Op\"].max().to_dict()\n",
        "job_ops = {j: v + 1 for j, v in job_ops.items()}\n",
        "job_data = {(int(r[\"Job\"]), int(r[\"Op\"])): (int(r[\"Machine\"]), int(r[\"Time\"])) for _, r in df.iterrows()}\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Config\n",
        "gamma = 0.95\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 0.995\n",
        "alpha = 0.001\n",
        "episodes = 20000\n",
        "batch_size = 128\n",
        "memory = deque(maxlen=50000)\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size = num_jobs + num_machines\n",
        "output_size = num_jobs\n",
        "\n",
        "q_net = QNetwork(input_size, output_size).to(device)\n",
        "optimizer = optim.Adam(q_net.parameters(), lr=alpha)\n",
        "\n",
        "def get_state_vector(job_progress, machine_status):\n",
        "    return np.array(job_progress + machine_status, dtype=np.float32)\n",
        "\n",
        "def get_feasible_actions(job_progress, machine_status, time):\n",
        "    return [j for j in range(num_jobs)\n",
        "            if job_progress[j] < job_ops[j]\n",
        "            and machine_status[job_data[(j, job_progress[j])][0]] <= time]\n",
        "\n",
        "def simulate_episode(train=True):\n",
        "    job_progress = [0] * num_jobs\n",
        "    machine_status = [0] * num_machines\n",
        "    schedule = []\n",
        "    time = 0\n",
        "    total_reward = 0\n",
        "\n",
        "    while any(job_progress[j] < job_ops[j] for j in range(num_jobs)):\n",
        "        state = get_state_vector(job_progress, machine_status)\n",
        "        feasible = get_feasible_actions(job_progress, machine_status, time)\n",
        "\n",
        "        if not feasible:\n",
        "            next_time = min([ms for ms in machine_status if ms > time], default=time + 1)\n",
        "            time = next_time\n",
        "            continue\n",
        "\n",
        "        state_tensor = torch.tensor(state, device=device)\n",
        "        if train and random.random() < epsilon:\n",
        "            action_job = random.choice(feasible)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_vals = q_net(state_tensor).cpu().numpy()\n",
        "            masked_q = np.full_like(q_vals, -np.inf)\n",
        "            for j in feasible:\n",
        "                masked_q[j] = q_vals[j]\n",
        "            action_job = int(np.argmax(masked_q))\n",
        "\n",
        "        op = job_progress[action_job]\n",
        "        m, pt = job_data[(action_job, op)]\n",
        "        start = max(machine_status[m], time)\n",
        "        end = start + pt\n",
        "\n",
        "        reward = -pt - (end - time) * 0.1\n",
        "        next_job_progress = job_progress.copy()\n",
        "        next_machine_status = machine_status.copy()\n",
        "        next_job_progress[action_job] += 1\n",
        "        next_machine_status[m] = end\n",
        "        next_state = get_state_vector(next_job_progress, next_machine_status)\n",
        "\n",
        "        if train:\n",
        "            memory.append(Transition(state, action_job, reward, next_state))\n",
        "\n",
        "        job_progress = next_job_progress\n",
        "        machine_status = next_machine_status\n",
        "        time = min(machine_status)\n",
        "        schedule.append((action_job, op, m, start, end))\n",
        "        total_reward += reward\n",
        "\n",
        "    makespan = max(e for *_, e in schedule) if schedule else 0\n",
        "    return total_reward, makespan, schedule\n",
        "\n",
        "# Training loop\n",
        "makespans = []\n",
        "schedules = []\n",
        "_, initial_makespan, _ = simulate_episode(train=False)\n",
        "makespans.append(initial_makespan)\n",
        "\n",
        "for ep in range(episodes):\n",
        "    _, mkspan, sched = simulate_episode(train=True)\n",
        "    makespans.append(mkspan)\n",
        "    schedules.append(sched)\n",
        "\n",
        "    if len(memory) >= batch_size:\n",
        "        batch = random.sample(memory, batch_size)\n",
        "        batch = Transition(*zip(*batch))\n",
        "\n",
        "        state_batch = torch.tensor(batch.state, device=device)\n",
        "        action_batch = torch.tensor(batch.action, device=device).unsqueeze(1)\n",
        "        reward_batch = torch.tensor(batch.reward, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "        next_state_batch = torch.tensor(batch.next_state, device=device)\n",
        "\n",
        "        q_values = q_net(state_batch).gather(1, action_batch)\n",
        "        with torch.no_grad():\n",
        "            q_next = q_net(next_state_batch).max(1)[0].unsqueeze(1)\n",
        "        target = reward_batch + gamma * q_next\n",
        "\n",
        "        loss = nn.MSELoss()(q_values, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "    if ep % 1000 == 0:\n",
        "        print(f\"Episode {ep}, Makespan: {mkspan}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "# Final Results\n",
        "best_idx = np.argmin(makespans)\n",
        "best_makespan = makespans[best_idx]\n",
        "best_schedule = schedules[best_idx]\n",
        "\n",
        "schedule_df = pd.DataFrame(best_schedule, columns=[\"Job ID\", \"Operation\", \"Machine\", \"Start\", \"End\"])\n",
        "schedule_df.sort_values(by=\"Start\", inplace=True)\n",
        "schedule_df[\"Current Makespan\"] = schedule_df[\"End\"].cummax()\n",
        "schedule_df[\"Contributes to Makespan\"] = schedule_df[\"End\"] == best_makespan\n",
        "used_time = schedule_df[\"End\"].sub(schedule_df[\"Start\"]).sum()\n",
        "idle_machines = num_machines * best_makespan - used_time\n",
        "utilization = used_time / (num_machines * best_makespan)\n",
        "\n",
        "print(\"\\nðŸ”§ Job-Machine-Time Assignments:\")\n",
        "print(schedule_df.to_string(index=False))\n",
        "print(f\"\\nðŸš€ Initial Random Makespan: {initial_makespan}\")\n",
        "print(f\"ðŸ“Š Total Optimized Makespan: {best_makespan}\")\n",
        "print(f\"ðŸ“‰ Makespan Reduced By: {initial_makespan - best_makespan} time units\")\n",
        "print(f\"ðŸ›  Idle Machine Time Units: {int(idle_machines)}\")\n",
        "print(f\"ðŸ“ˆ Machine Utilization: {utilization:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kizbNXU8f5ZV",
        "outputId": "5c21ad12-73ed-457d-d903-24cca1487c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Makespan: 45, Epsilon: 0.995\n",
            "Episode 1000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 2000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 3000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 4000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 5000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 6000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 7000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 8000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 9000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 10000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 11000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 12000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 13000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 14000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 15000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 16000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 17000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 18000, Makespan: 45, Epsilon: 0.050\n",
            "Episode 19000, Makespan: 45, Epsilon: 0.050\n",
            "\n",
            "ðŸ”§ Job-Machine-Time Assignments:\n",
            " Job ID  Operation  Machine  Start  End  Current Makespan  Contributes to Makespan\n",
            "      0          0        1      0    1                 1                    False\n",
            "      5          0        2      0    3                 3                    False\n",
            "      1          0        3      0    8                 8                    False\n",
            "      1          3        0      0   10                10                    False\n",
            "      1          5        4      0    4                10                    False\n",
            "      1          4        5      0   10                10                    False\n",
            "      3          0        1      1    6                10                    False\n",
            "      0          1        2      3    6                10                    False\n",
            "      3          2        4      4    9                10                    False\n",
            "      1          1        2      6   11                11                    False\n",
            "      1          2        1      6   16                16                    False\n",
            "      3          1        3      8   13                16                    False\n",
            "      5          4        4      9   13                16                    False\n",
            "      5          2        0     10   19                19                    False\n",
            "      4          2        5     10   15                19                    False\n",
            "      2          0        2     11   16                19                    False\n",
            "      2          5        4     13   20                20                    False\n",
            "      4          0        3     13   22                22                    False\n",
            "      3          5        5     15   24                24                    False\n",
            "      5          1        1     16   19                24                    False\n",
            "      3          3        2     16   19                24                    False\n",
            "      3          4        0     19   27                27                    False\n",
            "      2          1        1     19   23                27                    False\n",
            "      4          1        2     19   22                27                    False\n",
            "      0          3        4     20   27                27                    False\n",
            "      5          3        3     22   32                32                    False\n",
            "      4          4        1     23   26                32                    False\n",
            "      5          5        5     24   25                32                    False\n",
            "      2          3        5     25   34                34                    False\n",
            "      2          2        0     27   35                35                    False\n",
            "      4          5        4     27   28                35                    False\n",
            "      2          4        3     32   33                35                    False\n",
            "      0          2        3     33   39                39                    False\n",
            "      0          4        5     34   37                39                    False\n",
            "      0          5        0     35   41                41                    False\n",
            "      4          3        0     41   45                45                     True\n",
            "\n",
            "ðŸš€ Initial Random Makespan: 45\n",
            "ðŸ“Š Total Optimized Makespan: 45\n",
            "ðŸ“‰ Makespan Reduced By: 0 time units\n",
            "ðŸ›  Idle Machine Time Units: 73\n",
            "ðŸ“ˆ Machine Utilization: 72.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Final Optimized Version for FT10 or T10 Dataset\n",
        "# Uses smart reward + longer training + better epsilon\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "# Load FT10 or T10 Dataset\n",
        "df = pd.read_csv(\"/content/FT10_Dataset.csv\")  # change to T10_Dataset.csv if needed\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "df[\"Machine\"] = df[\"Machine\"].apply(lambda x: int(x[1:]) if isinstance(x, str) and x.startswith(\"M\") else int(x))\n",
        "df.rename(columns={\"Job ID\": \"Job\", \"Operation\": \"Op\", \"Machine\": \"Machine\", \"Processing Time\": \"Time\"}, inplace=True)\n",
        "\n",
        "num_jobs = df[\"Job\"].nunique()\n",
        "num_machines = df[\"Machine\"].nunique()\n",
        "job_ops = df.groupby(\"Job\")[\"Op\"].max().to_dict()\n",
        "job_ops = {j: v + 1 for j, v in job_ops.items()}\n",
        "job_data = {(int(r[\"Job\"]), int(r[\"Op\"])): (int(r[\"Machine\"]), int(r[\"Time\"])) for _, r in df.iterrows()}\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Config\n",
        "gamma = 0.95\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 0.999\n",
        "alpha = 0.0005\n",
        "episodes = 30000\n",
        "batch_size = 256\n",
        "memory = deque(maxlen=100000)\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size = num_jobs + num_machines\n",
        "output_size = num_jobs\n",
        "\n",
        "q_net = QNetwork(input_size, output_size).to(device)\n",
        "optimizer = optim.Adam(q_net.parameters(), lr=alpha)\n",
        "\n",
        "def get_state_vector(job_progress, machine_status):\n",
        "    return np.array(job_progress + machine_status, dtype=np.float32)\n",
        "\n",
        "def get_feasible_actions(job_progress, machine_status, time):\n",
        "    return [j for j in range(num_jobs)\n",
        "            if job_progress[j] < job_ops[j]\n",
        "            and machine_status[job_data[(j, job_progress[j])][0]] <= time]\n",
        "\n",
        "def simulate_episode(train=True):\n",
        "    job_progress = [0] * num_jobs\n",
        "    machine_status = [0] * num_machines\n",
        "    schedule = []\n",
        "    time = 0\n",
        "    total_reward = 0\n",
        "\n",
        "    while any(job_progress[j] < job_ops[j] for j in range(num_jobs)):\n",
        "        state = get_state_vector(job_progress, machine_status)\n",
        "        feasible = get_feasible_actions(job_progress, machine_status, time)\n",
        "\n",
        "        if not feasible:\n",
        "            time = min([ms for ms in machine_status if ms > time], default=time + 1)\n",
        "            continue\n",
        "\n",
        "        state_tensor = torch.tensor(state, device=device)\n",
        "        if train and random.random() < epsilon:\n",
        "            action_job = random.choice(feasible)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_vals = q_net(state_tensor).cpu().numpy()\n",
        "            masked_q = np.full_like(q_vals, -np.inf)\n",
        "            for j in feasible:\n",
        "                masked_q[j] = q_vals[j]\n",
        "            action_job = int(np.argmax(masked_q))\n",
        "\n",
        "        op = job_progress[action_job]\n",
        "        m, pt = job_data[(action_job, op)]\n",
        "        start = max(machine_status[m], time)\n",
        "        end = start + pt\n",
        "\n",
        "        next_job_progress = job_progress.copy()\n",
        "        next_machine_status = machine_status.copy()\n",
        "        next_job_progress[action_job] += 1\n",
        "        next_machine_status[m] = end\n",
        "        next_state = get_state_vector(next_job_progress, next_machine_status)\n",
        "\n",
        "        idle_penalty = sum([max(0, time - ms) for ms in machine_status])\n",
        "        reward = -pt - 0.3 * (start - time) - 0.05 * end - 0.1 * idle_penalty\n",
        "\n",
        "        if train:\n",
        "            memory.append(Transition(state, action_job, reward, next_state))\n",
        "\n",
        "        job_progress = next_job_progress\n",
        "        machine_status = next_machine_status\n",
        "        time = min(machine_status)\n",
        "        schedule.append((action_job, op, m, start, end))\n",
        "        total_reward += reward\n",
        "\n",
        "    makespan = max(e for *_, e in schedule) if schedule else 0\n",
        "    return total_reward, makespan, schedule\n",
        "\n",
        "# Training loop\n",
        "makespans = []\n",
        "schedules = []\n",
        "_, initial_makespan, _ = simulate_episode(train=False)\n",
        "makespans.append(initial_makespan)\n",
        "\n",
        "for ep in range(episodes):\n",
        "    _, mkspan, sched = simulate_episode(train=True)\n",
        "    makespans.append(mkspan)\n",
        "    schedules.append(sched)\n",
        "\n",
        "    if len(memory) >= batch_size:\n",
        "        batch = random.sample(memory, batch_size)\n",
        "        batch = Transition(*zip(*batch))\n",
        "\n",
        "        state_batch = torch.tensor(batch.state, device=device)\n",
        "        action_batch = torch.tensor(batch.action, device=device).unsqueeze(1)\n",
        "        reward_batch = torch.tensor(batch.reward, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "        next_state_batch = torch.tensor(batch.next_state, device=device)\n",
        "\n",
        "        q_values = q_net(state_batch).gather(1, action_batch)\n",
        "        with torch.no_grad():\n",
        "            q_next = q_net(next_state_batch).max(1)[0].unsqueeze(1)\n",
        "        target = reward_batch + gamma * q_next\n",
        "\n",
        "        loss = nn.MSELoss()(q_values, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "    if ep % 1000 == 0:\n",
        "        print(f\"Episode {ep}, Makespan: {mkspan}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "# Final Results\n",
        "best_idx = np.argmin(makespans)\n",
        "best_makespan = makespans[best_idx]\n",
        "best_schedule = schedules[best_idx]\n",
        "\n",
        "schedule_df = pd.DataFrame(best_schedule, columns=[\"Job ID\", \"Operation\", \"Machine\", \"Start\", \"End\"])\n",
        "schedule_df.sort_values(by=\"Start\", inplace=True)\n",
        "schedule_df[\"Current Makespan\"] = schedule_df[\"End\"].cummax()\n",
        "schedule_df[\"Contributes to Makespan\"] = schedule_df[\"End\"] == best_makespan\n",
        "used_time = schedule_df[\"End\"].sub(schedule_df[\"Start\"]).sum()\n",
        "idle_machines = num_machines * best_makespan - used_time\n",
        "utilization = used_time / (num_machines * best_makespan)\n",
        "\n",
        "print(\"\\nðŸ”§ Job-Machine-Time Assignments:\")\n",
        "print(schedule_df.to_string(index=False))\n",
        "print(f\"\\nðŸš€ Initial Random Makespan: {initial_makespan}\")\n",
        "print(f\"ðŸ“Š Total Optimized Makespan: {best_makespan}\")\n",
        "print(f\"ðŸ“‰ Makespan Reduced By: {initial_makespan - best_makespan} time units\")\n",
        "print(f\"ðŸ›  Idle Machine Time Units: {int(idle_machines)}\")\n",
        "print(f\"ðŸ“ˆ Machine Utilization: {utilization:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FPXvpnviSL0",
        "outputId": "72ff5634-ea2b-4f26-9f2d-d17eef43da41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Makespan: 704, Epsilon: 0.999\n",
            "Episode 1000, Makespan: 704, Epsilon: 0.367\n",
            "Episode 2000, Makespan: 704, Epsilon: 0.135\n",
            "Episode 3000, Makespan: 704, Epsilon: 0.050\n",
            "Episode 4000, Makespan: 704, Epsilon: 0.050\n",
            "Episode 5000, Makespan: 704, Epsilon: 0.050\n",
            "Episode 6000, Makespan: 704, Epsilon: 0.050\n",
            "Episode 7000, Makespan: 704, Epsilon: 0.050\n",
            "Episode 8000, Makespan: 704, Epsilon: 0.050\n"
          ]
        }
      ]
    }
  ]
}